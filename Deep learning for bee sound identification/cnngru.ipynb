{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b547f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_wbd = {}\n",
    "with open('/home/quanhhh/Documents/model/model_spec/sweep.yml', 'r') as f:\n",
    "    config_wdb = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_wdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, config, path):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.path = path\n",
    "        self.load_dataset()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {'input':self.data_[index], \n",
    "                'labels': self.labels[index]}\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        data = {}\n",
    "        path_ = self.config['data_dir'] + self.path\n",
    "        with np.load(path_, 'rb') as f:\n",
    "            data['data'] = f['data']\n",
    "            data['label'] = f['label']\n",
    "        self.data_ = data['data'].transpose(0,2,1)\n",
    "        self.data_ = np.expand_dims(self.data_, axis=1)\n",
    "        \n",
    "        \n",
    "        self.labels = data['label']\n",
    "    def getshape(self):\n",
    "        return self.data_.shape[1:]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceece34c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "# import torch.nn as nn\n",
    "class Conv_gru(nn.Module):\n",
    "    def __init__(self, shape_data, num_filters, kernel_size, maxpooling, d_hidden, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.shape_data = shape_data\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        # self.strides = strides\n",
    "        self.maxpooling = maxpooling\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # input -> conv2d -> batchnorm -> maxpool2d ->conv2d -> batchnorm -> maxpool \n",
    "        \n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                            in_channels=self.shape_data[0], \n",
    "                            out_channels=self.num_filters[0], \n",
    "                            kernel_size=self.kernel_size[0],\n",
    "                            )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "                            in_channels=self.num_filters[0], \n",
    "                            out_channels=self.num_filters[1], \n",
    "                            kernel_size=self.kernel_size[1],\n",
    "                            )\n",
    "\n",
    "        self.f_activation1 = nn.ReLU()\n",
    "        self.f_activation2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(num_features=self.num_filters[0])\n",
    "        self.batchNorm2 = nn.BatchNorm2d(num_features=self.num_filters[1])\n",
    "\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        \n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=self.maxpooling[0])\n",
    "        self.maxpooling2 = nn.MaxPool2d(kernel_size=self.maxpooling[1])\n",
    "\n",
    "    \n",
    "        self.h_conv1 = int((self.shape_data[1] - self.kernel_size[0]) + 1)\n",
    "        self.w_conv1 = int((self.shape_data[2] - self.kernel_size[0]) + 1)\n",
    "\n",
    "        self.h_maxpool1 = int((self.h_conv1 - self.maxpooling[0])/self.maxpooling[0] + 1)\n",
    "        self.w_maxpool1 = int((self.w_conv1 - self.maxpooling[0])/self.maxpooling[0] + 1)  \n",
    "\n",
    "        self.h_conv2 = int((self.h_maxpool1 - self.kernel_size[1]) + 1)\n",
    "        self.w_conv2 = int((self.w_maxpool1 - self.kernel_size[1]) + 1)\n",
    "\n",
    "        self.h_maxpool2 = int((self.h_conv2 - self.maxpooling[1])/self.maxpooling[1] + 1)\n",
    "        self.w_maxpool2 = int((self.w_conv2 - self.maxpooling[1])/self.maxpooling[1] + 1)  \n",
    "\n",
    "        self.gru = nn.GRU(self.w_maxpool2, self.d_hidden, self.num_layers, batch_first=True, dropout=self.dropout)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #layer1\n",
    "        self.conv1_out = self.conv1(x)\n",
    "        self.conv1relu_out = self.f_activation1(self.conv1_out)\n",
    "        \n",
    "        #layer2\n",
    "        \n",
    "        self.batchNorm1_out = self.batchNorm1(self.conv1relu_out)\n",
    "        self.maxpooling1_out = self.maxpooling1(self.batchNorm1_out)\n",
    "        \n",
    "        self.conv2_out = self.conv2(self.maxpooling1_out)\n",
    "        self.conv2relu_out = self.f_activation2(self.conv2_out)\n",
    "        \n",
    "        #layer3\n",
    "        self.batchNorm2_out = self.batchNorm2(self.conv2relu_out)\n",
    "        self.maxpooling2_out = self.maxpooling2(self.batchNorm2_out)\n",
    "        # self.outcnn = self.conv3(self.maxpooling2_out)\n",
    "\n",
    "        # # \n",
    "        # self.outcnn = torch.squeeze(self.outcnn, dim=1) \n",
    "        self.outcnn = torch.sum(self.maxpooling2_out, dim=1)/self.num_filters[1] #(batch, h, w)\n",
    "        # self.outcnn = torch.flatten(self.maxpooling2_out, start_dim=1, end_dim=2)\n",
    "        \n",
    "        #layer4\n",
    "        h_c = self.init_hidden(len(self.outcnn))\n",
    "        self.out, _ = self.gru(self.outcnn, h_c)\n",
    "        return self.softmax(self.out[:, -1, :])\n",
    "            \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.batch = batch_size\n",
    "        weight = next(self.gru.parameters()).data\n",
    "        \n",
    "        hidden = weight.new(self.num_layers, batch_size, self.d_hidden).zero_()        \n",
    "        # cell = weight.new(self.num_layers, batch_size, self.d_hidden).zero_()\n",
    "        \n",
    "        return hidden #, cell)\n",
    "    # def get_shape(self):\n",
    "    #     print('Input: [{}, {}, {}]'.format(\n",
    "    #         self.batch,\n",
    "    #         self.shape_data[0],\n",
    "    #         self.shape_data[1],\n",
    "    #     )\n",
    "    #     )\n",
    "    #     print('Conv1|Batch_norm1: [{}, {}, {}, {}] | Maxpool1: [{}, {}, {}, {}]'.format(\n",
    "    #         self.batch,\n",
    "    #         self.num_filters[0],\n",
    "    #         self.h_conv1,\n",
    "    #         self.w_conv1,\n",
    "    #         self.batch,\n",
    "    #         self.num_filters[0],\n",
    "    #         self.h_maxpool1,\n",
    "    #         self.w_maxpool1)\n",
    "    #         )\n",
    "    #     print('Conv2|Batch_norm1: [{}, {}, {}, {}] | Maxpool2: [{}, {}, {}, {}]'.format(\n",
    "    #         self.batch,\n",
    "    #         self.num_filters[1],\n",
    "    #         self.h_conv2,\n",
    "    #         self.w_conv2,\n",
    "    #         self.batch,\n",
    "    #         self.num_filters[0],\n",
    "    #         self.h_maxpool2,\n",
    "    #         self.w_maxpool2)\n",
    "    #         )\n",
    "    #     print('Custom layer: [{}, {}, {}]'.format(\n",
    "    #         self.batch,\n",
    "    #         self.h_maxpool2,\n",
    "    #         self.w_maxpool2)\n",
    "    #         )\n",
    "    #     print('Output: [{}, {}]'.format(\n",
    "    #         self.batch,\n",
    "    #         self.d_hidden)\n",
    "    #         )\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a06853",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36910b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch import optim\n",
    "# import time\n",
    "# import copy\n",
    "\n",
    "class ClassificationTrainer():\n",
    "    def __init__(self, model, train_data, val_data, val_set, device, config, buzz='buzz1'):\n",
    "        self.model = model\n",
    "    \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.val_set = val_set\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.max_acc = 95.67\n",
    "        self.max_val = 0\n",
    "        self.train_loss_list = list()\n",
    "        self.test_loss_list = list()\n",
    "        self.train_acc_list = list()\n",
    "        self.test_acc_list = list()\n",
    "        self.best_epoch = 0\n",
    "        self.best_model = None\n",
    "        self.buzz = buzz\n",
    "        self.path = self.config['result_dir'] + \"{}_model_{}num_layers_({},{})filters_({},{})kernel_size_({},{})maxpool_{}lr_{}epoch_{}batch_size.pt\".format(\n",
    "            self.buzz,\n",
    "            self.config['num_layers'],\n",
    "            self.config['num_filters_1'],\n",
    "            self.config['num_filters_2'],\n",
    "            self.config[\"kernel_size_1\"],\n",
    "            self.config[\"kernel_size_2\"],\n",
    "            # self.config[\"stride_1\"],\n",
    "            # self.config[\"stride_2\"],\n",
    "            self.config[\"maxpool_1\"],\n",
    "            self.config[\"maxpool_2\"],\n",
    "            self.config['lr'],\n",
    "            self.config['num_epoch'],\n",
    "            self.config['batch_size']\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self, criterion, opt, epoch):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        self.model.train()\n",
    "        lenght_train = 0\n",
    "        for i, data in enumerate(self.train_data):\n",
    "            input = data['input'].float().to(self.device)            \n",
    "            target = data['labels'].float().to(self.device)\n",
    "            out = self.model(input)\n",
    "            lenght_train += len(input)\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "            # print('\\rEpoch: {}\\t| Train_loss: {:.6f}\\t|Train accuracy: {:.6f}'.format(epoch, train_loss/lenght, train_acc/lenght), end='')\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.train_data):\n",
    "                input = data['input'].float().to(self.device)            \n",
    "                target = data['labels'].float().to(self.device)\n",
    "                \n",
    "                out = self.model(input)\n",
    "                \n",
    "                classes = torch.argmax(out, axis=1)\n",
    "                target_ = torch.argmax(target, axis=1)\n",
    "                #print(classes, target_)\n",
    "                train_acc += torch.sum(classes == target_)\n",
    "        self.train_loss = train_loss/lenght_train\n",
    "        self.train_acc = train_acc.item()*100/lenght_train\n",
    "        self.train_loss_list.append(self.train_loss)\n",
    "        self.train_acc_list.append(self.train_acc)\n",
    "        \n",
    "        \n",
    "        # print('\\nEpoch: {}\\t| Train_loss: {:.6f}\\t|Train accuracy: {:.6f}'.format(epoch, train_loss, train_acc))\n",
    "        if self.val_data is None:\n",
    "            if train_loss < self.min_loss:\n",
    "                self.min_loss = train_loss\n",
    "                self.best_epoch = epoch\n",
    "        else:\n",
    "            self.validate_epoch(criterion, epoch)\n",
    "        \n",
    "    def validate_epoch(self, criterion, epoch):\n",
    "        test_loss = 0.0\n",
    "        test_acc = 0.\n",
    "        val_acc = 0.0\n",
    "        lenght = 0\n",
    "        lenghtval = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.val_data):\n",
    "                input = data['input'].float().to(self.device)\n",
    "                target = data['labels'].float().to(self.device)\n",
    "                \n",
    "                out = self.model(input)\n",
    "                lenght += len(input)\n",
    "                classes = torch.argmax(out, axis=1)\n",
    "                target_ = torch.argmax(target, axis=1)\n",
    "                test_acc += torch.sum(classes == target_)\n",
    "                loss = criterion(out, target)\n",
    "                test_loss += loss.item()\n",
    "            \n",
    "            for i, data in enumerate(self.val_set):\n",
    "                input = data['input'].float().to(self.device)\n",
    "                target = data['labels'].float().to(self.device)\n",
    "                \n",
    "                out = self.model(input)\n",
    "                lenghtval += len(input)\n",
    "                \n",
    "                classes = torch.argmax(out, axis=1)\n",
    "                target_ = torch.argmax(target, axis=1)\n",
    "\n",
    "                val_acc += torch.sum(classes == target_)\n",
    "            \n",
    "        val_acc = val_acc*100/lenghtval  \n",
    "        test_loss = test_loss/lenght\n",
    "        test_acc = test_acc.item()*100/lenght\n",
    "        self.test_loss_list.append(test_loss)\n",
    "        self.test_acc_list.append(test_acc)\n",
    "        print('\\rEpoch: {}\\t| Train_loss: {:.6f}\\t|Train accuracy: {:.6f}\\t| Test_loss: {:.6f}\\t|Test accuracy: {:.6f}\\t|Val accuracy: {:.6f} '.format(\n",
    "                                                                                                                                        epoch, \n",
    "                                                                                                                                        self.train_loss, \n",
    "                                                                                                                                        self.train_acc, \n",
    "                                                                                                                                        test_loss, \n",
    "                                                                                                                                        test_acc, \n",
    "                                                                                                                                        val_acc), \n",
    "                                                                                                                                        end=\"\\r\") \n",
    "        wandb.log({'train_loss': self.train_loss, 'test_loss': test_loss})\n",
    "        if val_acc >= self.max_val:\n",
    "            self.max_val = val_acc\n",
    "            self.best_epoch = epoch\n",
    "            if self.max_val >= 97.6:\n",
    "                self.best_model_path = self.config['result_dir'] + \"{}_model_{}num_layers_({},{})filters_({},{})kernel_size_({},{})maxpool_{}lr_{}epoch_{}batch_size.pt\".format(\n",
    "                    self.buzz,\n",
    "                    self.config['num_layers'],\n",
    "                    self.config['num_filters_1'],\n",
    "                    self.config['num_filters_2'],\n",
    "                    self.config[\"kernel_size_1\"],\n",
    "                    self.config[\"kernel_size_2\"],\n",
    "                    # self.config[\"stride_1\"],\n",
    "                    # self.config[\"stride_2\"],\n",
    "                    self.config[\"maxpool_1\"],\n",
    "                    self.config[\"maxpool_2\"],\n",
    "                    self.config['lr'],\n",
    "                    epoch,\n",
    "                    self.config['batch_size']\n",
    "                )\n",
    "                self.best_model = copy.deepcopy(self.model)\n",
    "            # torch.save(self.model.state_dict(), path)\n",
    "    def train(self):\n",
    "        self.model.to(self.device)  \n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        model_opt = optim.Adam(self.model.parameters(), self.config['lr'], weight_decay = self.config['weight_decay'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        print(\"-----START TRAINING CLASSIFICATION-----\")\n",
    "        for epoch in range(1, self.config['num_epoch'] + 1):\n",
    "            self.train_epoch(criterion, model_opt, epoch)  \n",
    "        print('\\n')\n",
    "        self.config['train_time'] = (time.perf_counter() - start)/60\n",
    "        print(\"----COMPLETED TRAINING CLASSIFICATION-----\")\n",
    "\n",
    "        self.config[\"best_epoch\"] = self.best_epoch\n",
    "        if self.best_model is not None:\n",
    "            torch.save(self.best_model.state_dict(), self.best_model_path)\n",
    "            self.config['best_model_path'] = self.best_model_path\n",
    "        else:\n",
    "            self.config['best_model_path'] = ''\n",
    "\n",
    "        self.config['val_acc'] = self.max_val\n",
    "        self.config['train_loss'] = self.train_loss_list\n",
    "        self.config['train_acc'] = self.train_acc_list\n",
    "        self.config['test_loss'] = self.test_loss_list\n",
    "        self.config['test_accuracy'] = self.test_acc_list\n",
    "        # self.config['model_path'] = self.path \n",
    "        # self.save_loss()\n",
    "\n",
    "    def get_updated_config(self):\n",
    "        return self.config\n",
    "    \n",
    "    def load_model(self, path = None):\n",
    "        if path is None:\n",
    "            path = self.config['result_dir'] + \"model.pt\"\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8dcd2",
   "metadata": {},
   "source": [
    "# Train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=config_wdb, project='quanh')\n",
    "# sweep_id = \"9g9inm8n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40727418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "def trainwandb():\n",
    "    with wandb.init(config=config_wbd):\n",
    "        config = wandb.config\n",
    "        # train_set = CustomDataset(config, 'buzz1_traindata.npz')\n",
    "        # shape = train_set.getshape()\n",
    "        # data_dataloader = DataLoader(train_set, \n",
    "        #                               batch_size = 128,\n",
    "        #                               shuffle = True)\n",
    "        # train_dataloader = []\n",
    "        # test_dataloader = []\n",
    "        # for i, batch in enumerate(data_dataloader):\n",
    "        #     if i < len(data_dataloader)*0.7:\n",
    "        #         train_dataloader.append(batch)\n",
    "        #     else:\n",
    "        #         test_dataloader.append(batch)\n",
    "\n",
    "        #buzz2\n",
    "        train_set = CustomDataset(config, 'buzz2_vn_traindata.npz')\n",
    "        train_dataloader = DataLoader(train_set, \n",
    "                                    batch_size = 128,\n",
    "                                    shuffle = True)\n",
    "        shape = train_set.getshape()\n",
    "        val_set = CustomDataset(config, 'buzz2_vn_testdata.npz')\n",
    "        test_dataloader = DataLoader(val_set,\n",
    "                                    batch_size = 128,\n",
    "                                    shuffle = True)\n",
    "        ###\n",
    "        val_set = CustomDataset(config, 'buzz2_vn_valdata.npz')\n",
    "        val_set_ = DataLoader(val_set,\n",
    "                                    batch_size = 128,\n",
    "                                    shuffle = True)\n",
    "        torch.manual_seed(10)\n",
    "        classification_model = Conv_gru(shape_data=shape, \n",
    "                            num_filters=(config['num_filters_1'], config['num_filters_2']),\n",
    "                            kernel_size=(config['kernel_size_1'], config['kernel_size_2']), \n",
    "                            # strides=(config['stride_1'], config['stride_2']), \n",
    "                            maxpooling=(config['maxpool_1'], config['maxpool_2']), \n",
    "                            d_hidden=config['d_hidden'], \n",
    "                            num_layers= config['num_layers'])\n",
    "        classification_model.float()\n",
    "        classification_trainer = ClassificationTrainer(model=classification_model,\n",
    "                                                    train_data=train_dataloader,\n",
    "                                                    val_data=test_dataloader,\n",
    "                                                    val_set=val_set_,\n",
    "                                                    device=device,\n",
    "                                                    config=config,\n",
    "                                                    buzz=\"buzz2_vn\")\n",
    "        classification_trainer.train()\n",
    "        config = classification_trainer.get_updated_config()\n",
    "        val_acc = config['val_acc']\n",
    "        wandb.log({'val_acc': val_acc})\n",
    "        \n",
    "        if val_acc >= 97.6:\n",
    "            wandb.alert(\n",
    "                title=f\"MaiAnh found new high accuracy ID {sweep_id}\", \n",
    "                text=f\"Accuracy {val_acc} \"\n",
    "            )\n",
    "#         filename = \"{}num_layers_({},{})kernel_size_{}n_filters_({},{})stride_({},{})maxpool_{}lr_{}batch_size\".format(\n",
    "#             config[\"num_layers\"],\n",
    "#             config[\"kernel_size_1\"],\n",
    "#             config[\"kernel_size_2\"],\n",
    "#             config[\"num_filters\"],\n",
    "#             config[\"stride_1\"],\n",
    "#             config[\"stride_2\"],\n",
    "#             config[\"maxpool_1\"],\n",
    "#             config[\"maxpool_2\"],\n",
    "#             config['lr'],\n",
    "#             config['batch_size']\n",
    "#         ).replace(\".\", \"_\")\n",
    "#         PATH = os.path.join(\"configs_wandb/\", \"{}.yml\".format(filename))\n",
    "#         with open(PATH, 'w') as outfile:\n",
    "#             yaml.dump(config, outfile, default_flow_style=False)\n",
    "        print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ad92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=trainwandb, count=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "99dd562274464ac4f8f08ae1dd21875d60a3f78d363ee37c9f3e501c0671512c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
